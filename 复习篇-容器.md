# 容器

## Map

### HashMap

**static final int DEFAULT_INITIAL_CAPACITY = 1 << 4：**默认的table初始容量

**static final float DEFAULT_LOAD_FACTOR = 0.75f：**默认的负载因子

**static final int TREEIFY_THRESHOLD = 8:** 链表长度大于该参数转红黑树

**static final int UNTREEIFY_THRESHOLD = 6:** 当树的节点数小于等于该参数转成链表

**1. 存储结构**

内部包含了一个Node类型的数组table。Node存储着键值对，包含了四个字段，从next字段可以看出Node是一个链表，即数组中的每个位置会被当成一个桶，一个桶存放一个链表。HashMap使用拉链法来解决冲突，同一个链表中存放**哈希值和散列桶取模运算结果相同**的Node。当链表长度大于8，会转换为红黑树(JDK 1.8)。

```java
transient Node<K, V>[] table;
```

```java
/**
* Basic hash bin node, used for most entries.
*/
static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
        V value;
        Node<K,V> next;

        Node(int hash, K key, V value, Node<K,V> next) {
        }
    
        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }

        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }

        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Map.Entry<?,?> e = (Map.Entry<?,?>)o;
                if (Objects.equals(key, e.getKey()) &&
                    Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
}
```

**2. 确定桶下标，get操作**

```java
public V get(Object key) {
    Node<K,V> e;
    return (e = getNode(hash(key), key)) == null ? null : e.value;
}

final Node<K,V> getNode(int hash, Object key) {
        Node<K,V>[] tab; Node<K,V> first, e; int n; K k;
    	// 如果table数组不为空，并且要查找索引处有值，就判断位于第一个的key是否是要查找的key
        if ((tab = table) != null && (n = tab.length) > 0 &&
            (first = tab[(n - 1) & hash]) != null) {
            // 查找第一个key是否是目标key
            if (first.hash == hash && // always check first node
                ((k = first.key) == key || (key != null && key.equals(k))))
                return first;
            
            if ((e = first.next) != null) {
                // 判断链表是否是红黑树，如果是，就从树中取值
                if (first instanceof TreeNode)
                    return ((TreeNode<K,V>)first).getTreeNode(hash, key);
                // 不是红黑树，遍历链表
                do {
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        return e;
                } while ((e = e.next) != null);
            }
        }
        return null;
}

static final int hash(Object key) {
     int h;														 // hashCode()是native方法
     return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); // >>> 无符号右移, ^ 异或
}
```

由源码可知，HashMap是通过key的hashCode和高16位异或后和桶的数量取模得到索引位置，即 `index = key.hashCode()^(hashcode >>> 16)%table.length`；当length是2^n时，h&(length - 1)运算等价于h%length，而 & 操作比%效率更高。而采用高16位和低16位进行异或，也可以让所有的位数都参与运算，使得在length比较小的时候也可以做到尽量的散列。(由于和（length-1）运算，length 绝大多数情况小于2的16次方。所以始终是hashcode 的低16位（甚至更低）参与运算。要是高16位也参与运算，会让得到的下标更加散列。)

**为什么是2^n？**

1、当数组长度为2的n次幂的时候，不同的key算得index相同的几率较小，数据在数组上分布就比较均匀，即碰撞几率小，查询效率高。

2、在扩容的时候，如果length每次是2^n，那么重新计算出来的索引只有两种情况：一种是old索引+原数组长度 ，另一种就是索引不变，所以就不需要每次重新计算索引。为什么？(a)： 扩容前；(b)： 扩容后

![img](https://img-blog.csdnimg.cn/20190127173346891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxMzI5MzM0MzI=,size_16,color_FFFFFF,t_70)

![img](https://img-blog.csdnimg.cn/20190127173447456.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxMzI5MzM0MzI=,size_16,color_FFFFFF,t_70)

因此，我们在扩充HashMap的时候，不需要像1.7重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap(oldCap是原数组的容量大小)”

3、当length是2^n时，h&（length-1）运算等价于h%length，而&操作比%效率更高。

**3、put操作**

```java
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}

final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) {
        Node<K,V>[] tab; Node<K,V> p; int n, i;
    
    // 判断table是否为空，若为空则创建一个table，并获取长度
        if ((tab = table) == null || (n = tab.length) == 0)
            n = (tab = resize()).length;
    // 计算出来的数组索引没有放入数据，直接创建新节点后放入
        if ((p = tab[i = (n - 1) & hash]) == null)
            tab[i] = newNode(hash, key, value, null);
        else {
            Node<K,V> e; K k;
           // 判断put的数据是否是table数组index处的第一个节点
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
            // 如果是红黑树就直接插入树中
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            //如果是链表，就遍历每个节点，判断链表长度是否大于8，如果大于就转换为红黑树
            else {
                for (int binCount = 0; ; ++binCount) {
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            //如果e不是null，说明没有迭代到最后就跳出了循环，说明链表中有相同的key，因此只需要将value覆盖，并将oldValue返回即可
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount;
        if (++size > threshold)
            resize();
        afterNodeInsertion(evict);
        return null;
}
```

1.8中插入数据使用尾插法，而1.7使用的是头插法；1.8 中，**链表长度大于8，转化为红黑树存储**。HashMap采用hash算法来决定Map中key的存储，并通过hash算法来增加集合的大小。hash表里可以存储元素的位置称为桶（bucket），如果通过key计算hash值发生冲突时，那么将采用链表的形式，来存储元素。

头插法：考虑到**热点数据**，即最近插入的元素也很可能最近会被使用到。所以为了缩短链表查找元素的时间，所以每次都会将新插入的元素放到表头。

但是头插法，在并发扩容时可能导致死循环。---见扩容部分。

**4、扩容**

```shell
参考链接：https://www.cnblogs.com/Xieyang-blog/p/8886921.html
```

```java
// jkd1.7 核心扩容方法
void transfer(Entry[] newTable, boolean rehash) {
        int newCapacity = newTable.length;
        for (Entry<K,V> e : table) {

            while(null != e) {
                Entry<K,V> next = e.next;
                if (rehash) {
                    e.hash = null == e.key ? 0 : hash(e.key);
                }
                int i = indexFor(e.hash, newCapacity);
                e.next = newTable[i];
                newTable[i] = e;
                e = next;
            }
        }
    }
```

```shell
# 头插法并发扩容时发生死锁的过程：
	jdk1.7 时，transfer将当前数组中各节点e移动到新数组的i位置上时，为了避免调用put方法，直接取e.next = newTable[i];
	举例：oldtable[ i ]为：A->B->null     newtable[ j ]为：X->Y->null
	正常扩容后table数组为：最后newTable[ j ]结果为：B->A->X->Y->null，变成了逆序。
	若并发发生时：
	假如线程1在刚执行完 Entry<K,V> next = e.next;  后，此时e指向A，next指向B，
	然后被B线程抢占，然后B完整执行完扩容后，此时newTable[ j ]为：B->A->X->Y->null，
	然后A线程恢复执行，e.next = newTable[i];，此时newTable[ j ]为：B<->A ；newTable[i] = e;，此时newTable[ j ]为：A<->B 
	继续循环，发现已经形成环，没有null了， while(null != e) 永远跳不出循环，所以会形成死锁。
```

```java
// jkd1.8 核心扩容方法
final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        int oldThr = threshold;
        int newCap, newThr = 0;
        if (oldCap > 0) {
            // 超过最大容量，不扩容
            if (oldCap >= MAXIMUM_CAPACITY) {
                threshold = Integer.MAX_VALUE;
                return oldTab;
            }
            //容量没有超过最大值，容量变为原来的两倍
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
                newThr = oldThr << 1; // double threshold
        }
        else if (oldThr > 0) // initial capacity was placed in threshold
            newCap = oldThr;
        else {               // zero initial threshold signifies using defaults
            newCap = DEFAULT_INITIAL_CAPACITY;
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
        }
        if (newThr == 0) {
            float ft = (float)newCap * loadFactor;
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
        threshold = newThr;
        @SuppressWarnings({"rawtypes","unchecked"})
    	//创建新的 Hash 表
        Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
        table = newTab;
        if (oldTab != null) {
            for (int j = 0; j < oldCap; ++j) {
                Node<K,V> e;
                if ((e = oldTab[j]) != null) {
                    oldTab[j] = null;
                    //当前节点不是以链表的形式存在
                    if (e.next == null)
                        newTab[e.hash & (newCap - 1)] = e;
                    // 红黑树的形式，略过
                    else if (e instanceof TreeNode)
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                    else { // preserve order
                        
                        Node<K,V> loHead = null, loTail = null;
                        Node<K,V> hiHead = null, hiTail = null;
                        Node<K,V> next;
                        do {
                            next = e.next;
                            if ((e.hash & oldCap) == 0) {
                                if (loTail == null)
                                    loHead = e;
                                else
                                    loTail.next = e;
                                loTail = e;
                            }
                            else {
                                if (hiTail == null)
                                    hiHead = e;
                                else
                                    hiTail.next = e;
                                hiTail = e;
                            }
                        } while ((e = next) != null);
                        if (loTail != null) {
                            loTail.next = null;
                            newTab[j] = loHead;
                        }
                        if (hiTail != null) {
                            hiTail.next = null;
                            newTab[j + oldCap] = hiHead;
                        }
                    }
                }
            }
        }
        return newTab;
}
```

扩容总结：**lo就是扩容后仍然在原地的元素链表，hi就是扩容后下标为 原位置+原容量 的元素链表，从而不需要重新计算hash。**需要判断左边新增的那一位（右数第5位）是否为1即可判断此节点是留在原地lo还是移动去高位hi：(e.hash & oldCap) == 0 。(该部分在确定哈希值的时候就已经介绍了)

利用了尾指针Tail，完成了尾部插入，不会造成逆序，所以也不会产生并发死锁的问题。

这种方法**对比1.7中算法的优点是**：

1、不管怎么样都不需要重新再计算hash；

2、放过去的链表内元素的相对顺序不会改变；

3、不会在并发扩容中发生死锁。

注意，**时间复杂度并没有减少**



### ConcurrentHashMap

```shell
参考博客：https://www.jianshu.com/p/460bf90b9137
```

#### jdk1.7

ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入锁；HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组，Segment是一种数组和链表结构。一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得与它对应的Segment锁。

#### jdk1.8

**1、存储结构**

使用`CAS + Synchronized`，对每个数组元素加锁；

使用了`CAS`操作来支持更高的并发度，在`CAS`操作失败时使用内置锁synchronized。

节点Node其中的`val、next`都使用了volatile，保证了可见性；

![JDK1.8的ConcurrentHashMap](https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-6/JDK1.8-ConcurrentHashMap-Structure.jpg)

```java
// table数组的最大容量和默认容量
private static final int MAXIMUM_CAPACITY = 1 << 30;
private static final int DEFAULT_CAPACITY = 16; 

static final int TREEIFY_THRESHOLD = 8; 
static final int UNTREEIFY_THRESHOLD = 6; 

// 当table数组的长度小于此值时，不会把链表转化为红黑树。
static final int MIN_TREEIFY_CAPACITY = 64;
static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; 
private static final int DEFAULT_CONCURRENCY_LEVEL = 16; 
private static final float LOAD_FACTOR = 0.75f; 

// 扩容操作中，transfer允许多线程，这个常量表示一个线程执行transfer时，最少要对连续的16个hash桶进行transfer，不足16就按16算
// 也就是单线程执行transfer时的最小任务量，单位为一个hash桶，这就是线程的transfer的步进（stride）
private static final int MIN_TRANSFER_STRIDE = 16; 

// 用于生成每次扩容都唯一的生成戳的数。
private static int RESIZE_STAMP_BITS = 16; 

// 最大的扩容线程的数量
private static final int MAX_RESIZERS = (1 << (32 - RESIZE_STAMP_BITS)) - 1;

// 移位量，把生成戳移位后保存在sizeCtl中当做扩容线程计数的基数，相反方向移位后能够反解出生成戳
private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS; 

// 下面几个是特殊的节点的hash值，正常节点的hash值在hash函数中都处理过了，不会出现负数的情况，特殊节点在各自的实现类中有特殊的遍历方法
// ForwardingNode的hash值，ForwardingNode是一种临时节点，在扩容进行中才会出现，并且它不存储实际的数据
// 如果旧数组的一个hash桶中全部的节点都迁移到新数组中，旧数组就在这个hash桶中放置一个ForwardingNode
// 读操作或者迭代读时碰到ForwardingNode时，将操作转发到扩容后的新的table数组上去执行，写操作碰见它时，则尝试帮助扩容
static final int MOVED = -1; 

// TreeBin的hash值，TreeBin是ConcurrentHashMap中用于代理操作TreeNode的特殊节点，持有存储实际数据的红黑树的根节点
// 因为红黑树进行写入操作，整个树的结构可能会有很大的变化，这个对读线程有很大的影响，
// 所以TreeBin还要维护一个简单读写锁，这是相对HashMap，这个类新引入这种特殊节点的重要原因
static final int TREEBIN   = -2; 

// ReservationNode的hash值，ReservationNode是一个保留节点，就是个占位符，不会保存实际的数据，正常情况是不会出现的
static final int RESERVED  = -3; 

// 用于和负数hash值进行 & 运算，将其转化为正数（绝对值不相等），Hashtable中定位hash桶也有使用这种方式来进行负数转正数
static final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash 
```

```java
transient volatile Node<K,V>[] table;

// 扩容后的新的table数组，只有在扩容时才有用
// nextTable != null，说明扩容方法还没有真正退出，一般可以认为是此时还有线程正在进行扩容
private transient volatile Node<K,V>[] nextTable;

//控制table的初始化和扩容
//值为负时，表示正在进行初始化或扩容：
// sizeCtl = -1，表示有线程正在进行真正的初始化操作
// sizeCtl = -(1 + nThreads)，表示有nThreads个线程正在进行扩容操作
// sizeCtl > 0，表示接下来的真正的初始化操作中使用的容量，或者初始化/扩容完成后的threshold
// sizeCtl = 0，默认值，此时在真正的初始化操作中使用默认容量
private transient volatile int sizeCtl;

// 下一个transfer任务的起始下标index 加上1 之后的值，transfer时下标index从length - 1开始往0走
// transfer时方向是倒过来的，迭代时是下标从小往大，二者方向相反，尽量减少扩容时transefer和迭代两者同时处理一个hash桶的情况
// 顺序相反时，二者相遇过后，迭代没处理的都是已经transfer的hash桶，transfer没处理的，都是已经迭代的hash桶，冲突会变少
// 下标在[nextIndex - 实际的stride （下界要 >= 0）, nextIndex - 1]内的hash桶，就是每个transfer的任务区间
// 每次接受一个transfer任务，都要CAS执行 transferIndex = transferIndex - 实际的stride
// 保证一个transfer任务不会被几个线程同时获取（相当于任务队列的size减1）
// 当没有线程正在执行transfer任务时，一定有transferIndex <= 0，这是判断是否需要帮助扩容的重要条件（相当于任务队列为空）
private transient volatile int transferIndex;
```

```java
static class Node<K,V> implements Map.Entry<K,V> {
    final int hash;
    final K key;
    volatile V val;
    volatile Node<K,V> next;


    public final int hashCode()   { return key.hashCode() ^ val.hashCode(); }
   

    public final boolean equals(Object o) {
        Object k, v, u; Map.Entry<?,?> e;
        return ((o instanceof Map.Entry) &&
                (k = (e = (Map.Entry<?,?>)o).getKey()) != null &&
                (v = e.getValue()) != null &&
                (k == key || k.equals(key)) &&
                (v == (u = val) || v.equals(u)));
    }

}

//ConcurrentHashMap对此节点的操作，都会由TreeBin来代理执行
static final class TreeNode<K,V> extends Node<K,V> {
    TreeNode<K,V> parent;  // red-black tree links
    TreeNode<K,V> left;
    TreeNode<K,V> right;
    TreeNode<K,V> prev;    // needed to unlink next upon deletion
    boolean red;

}
```

```java
//ForwardingNode是一种临时节点，在扩容进行中才会出现，hash值固定为-1，并且它不存储实际的数据。
//如果旧数组的一个hash桶中全部的节点都迁移到新数组中，旧数组就在这个hash桶中放置一个ForwardingNode。
//读操作或者迭代读时碰到ForwardingNode时，将操作转发到扩容后的新的table数组上去执行，
//写操作碰见它时，则尝试帮助扩容。
static final class ForwardingNode<K,V> extends Node<K,V> {
    final Node<K,V>[] nextTable;
    ForwardingNode(Node<K,V>[] tab) {
        super(MOVED, null, null, null);
        this.nextTable = tab;
    }

}
```

```java
//TreeBin的hash值固定为-2，它是ConcurrentHashMap中用于代理操作TreeNode的特殊节点，持有存储实际数据的红黑树的根节点。
//因为红黑树进行写入操作，整个树的结构可能会有很大的变化，这个对读线程有很大的影响，所以TreeBin还要维护一个简单读写锁，这是相对HashMap，这个类新引入这种特殊节点的重要原因。
static final class TreeBin<K,V> extends Node<K,V> {
    TreeNode<K,V> root;
    volatile TreeNode<K,V> first;
    volatile Thread waiter;
    volatile int lockState;

    // 红黑树的 读、写锁状态 是互斥的，指的是以红黑树方式进行的读操作和写操作（只有部分的put/remove需要加写锁）是互斥的    
    // 但是当有线程持有红黑树的 写锁 时，读线程不会以红黑树方式进行读取操作，而是使用简单的链表方式进行读取，此时读操作和写操作可以并发执行    
    // 当有线程持有红黑树的 读锁 时，写线程可能会阻塞，不过因为红黑树的查找很快，写线程阻塞的时间很短    
    // 另外一点，ConcurrentHashMap的put/remove/replace方法本身就会锁住TreeBin节点，这里不会出现写-写竞争的情况，因此这里的读写锁可以实现得很简单     

    static final int WRITER = 1; // set while holding write lock
    static final int WAITER = 2; // set when waiting for write lock
    static final int READER = 4; // increment value for setting read lock

    // 在hashCode相等并且不是Comparable类时才使用此方法进行判断大小
    static int tieBreakOrder(Object a, Object b) {
        int d;
        if (a == null || b == null ||
            (d = a.getClass().getName().
             compareTo(b.getClass().getName())) == 0)
            d = (System.identityHashCode(a) <= System.identityHashCode(b) ?
                 -1 : 1);
        return d;
    }


    // 对根节点加 写锁，红黑树重构时需要加上 写锁
    private final void lockRoot() {
        if (!U.compareAndSwapInt(this, LOCKSTATE, 0, WRITER))
            contendedLock(); // offload to separate method
    }

    //释放写锁
    private final void unlockRoot() {
        lockState = 0;
    }



    // 从根节点开始遍历查找，找到“相等”的节点就返回它，没找到就返回null
    // 当有写线程加上 写锁 时，使用链表方式进行查找
    final Node<K,V> find(int h, Object k) {
        if (k != null) {
            for (Node<K,V> e = first; e != null; ) {
                int s; K ek;   
                
                // 两种特殊情况下以链表的方式进行查找
                // 1、有线程正持有 写锁，这样做能够不阻塞读线程
                // 2、WAITER时，不再继续加 读锁，能够让已经被阻塞的写线程尽快恢复运行，或者刚好让某个写线程不被阻塞
                if (((s = lockState) & (WAITER|WRITER)) != 0) {
                    if (e.hash == h &&
                        ((ek = e.key) == k || (ek != null && k.equals(ek))))
                        return e;
                    e = e.next;
                }
                else if (U.compareAndSwapInt(this, LOCKSTATE, s,
                                             s + READER)) {
                    TreeNode<K,V> r, p;
                    try {
                        p = ((r = root) == null ? null :
                             r.findTreeNode(h, k, null));
                    } finally {
                        Thread w;
                        if (U.getAndAddInt(this, LOCKSTATE, -READER) ==
                            (READER|WAITER) && (w = waiter) != null)
                            LockSupport.unpark(w);
                    }
                    return p;
                }
            }
        }
        return null;
    }

}
```

**2、一些基本操作**

```java
// 下面几个用于读写table数组，使用Unsafe提供的更强的功能（数组元素的volatile读写，CAS 更新）代替普通的读写，调用者预先进行参数控制

// volatile读取table[i]
@SuppressWarnings("unchecked")
static final <K,V> Node<K,V> tabAt(Node<K,V>[] tab, int i) {    
    return (Node<K,V>)U.getObjectVolatile(tab, ((long)i << ASHIFT) + ABASE);
}

// CAS更新table[i]，也就是Node链表的头节点，或者TreeBin节点（它持有红黑树的根节点）
static final <K,V> boolean casTabAt(Node<K,V>[] tab, int i, Node<K,V> c, Node<K,V> v) { 
    return U.compareAndSwapObject(tab, ((long)i << ASHIFT) + ABASE, c, v);
}

// volatile写入table[i]
static final <K,V> void setTabAt(Node<K,V>[] tab, int i, Node<K,V> v) { 
    U.putObjectVolatile(tab, ((long)i << ASHIFT) + ABASE, v);
} 

// 满足变换为红黑树的两个条件时（链表长度这个条件调用者保证，这里只验证Map容量这个条件），将链表变为红黑树，否则只是进行一次扩容操作
private final void treeifyBin(Node<K,V>[] tab, int index) {    
    Node<K,V> b; int n, sc;    
    if (tab != null) {        
        if ((n = tab.length) < MIN_TREEIFY_CAPACITY) 
            // Map的容量不够时，只是进行一次扩容            
            tryPresize(n << 1);        
        else if ((b = tabAt(tab, index)) != null && b.hash >= 0) {           
            synchronized (b) {                
                if (tabAt(tab, index) == b) {                    
                    TreeNode<K,V> hd = null, tl = null;                    
                    for (Node<K,V> e = b; e != null; e = e.next) {
                        TreeNode<K,V> p = new TreeNode<K,V>(e.hash, e.key, e.val, null, null);  
                        if ((p.prev = tl) == null)                            
                            hd = p;                        
                        else                            
                            tl.next = p;                        
                        tl = p;                    
                    }                    
                    setTabAt(tab, index, new TreeBin<K,V>(hd));                
                }            
            }        
        }    
    }
} 

// 规模不足时把红黑树转化为链表，此方法由调用者进行synchronized加锁，所以这里不加锁
static <K,V> Node<K,V> untreeify(Node<K,V> b) {    
    Node<K,V> hd = null, tl = null;    
    for (Node<K,V> q = b; q != null; q = q.next) {        
        Node<K,V> p = new Node<K,V>(q.hash, q.key, q.val, null);        
        if (tl == null)            
            hd = p;        
        else            
            tl.next = p;        
        tl = p;    
    }    
    return hd;
}
```

**3、put() 方法**

```java
final V putVal(K key, V value, boolean onlyIfAbsent) {
    if (key == null || value == null) throw new NullPointerException();
    //1、根据 key 计算出 hashcode
    int hash = spread(key.hashCode());
    int binCount = 0;
    for (Node<K,V>[] tab = table;;) {
        Node<K,V> f; int n, i, fh;
        //2、判断是否需要进行初始化
        if (tab == null || (n = tab.length) == 0)
            tab = initTable();
    //3、f即为当前key定位出的Node，如果为空表示当前位置可以写入数据，利用CAS尝试写入，失败则自旋保证成功。
        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
            if (casTabAt(tab, i, null,
                         new Node<K,V>(hash, key, value, null)))
                break;                  
        }
        //4、如果当前位置的hashcode == MOVED == -1,则需要进行扩容。如果当前链表已经迁移完成，那么头节点会被设置成fwd节点，此时写线程会帮助扩容；如果扩容没有完成，当前链表的头节点会被锁住，所以写线程会被阻塞，直到扩容完成。
        else if ((fh = f.hash) == MOVED)
            tab = helpTransfer(tab, f);
        else {
             /*
                 * 如果在这个位置有元素的话，就采用synchronized的方式加锁，
                 *     如果是链表的话(hash大于0)，就对这个链表的所有元素进行遍历，
                 *         如果找到了key和key的hash值都一样的节点，则把它的值替换
                 *         如果没找到的话，则添加在链表的最后面
                 *  否则，是树的话，则调用putTreeVal方法添加到树中去
                 *  
                 *  在添加完之后，会对该节点上关联的的数目进行判断，
                 *  如果在8个以上的话，则会调用treeifyBin方法，来尝试转化为树，或者是扩容
                 */
            V oldVal = null;
            //5、如果都不满足，则利用synchronized锁写入数据。
            synchronized (f) {
                if (tabAt(tab, i) == f) {
                    //6、当前为链表，在链表中插入新的键值对
                    if (fh >= 0) {
                        binCount = 1;
                        for (Node<K,V> e = f;; ++binCount) {
                            K ek;
                            if (e.hash == hash &&
                                ((ek = e.key) == key ||
                                 (ek != null && key.equals(ek)))) {
                                oldVal = e.val;
                                if (!onlyIfAbsent)
                                    e.val = value;
                                break;
                            }
                            Node<K,V> pred = e;
                            if ((e = e.next) == null) {
                                pred.next = new Node<K,V>(hash, key,
                                                          value, null);
                                break;
                            }
                        }
                    }
                    //7、当前为红黑树，将新的键值对插入到红黑树中
                    else if (f instanceof TreeBin) {
                        Node<K,V> p;
                        binCount = 2;
                        if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                       value)) != null) {
                            oldVal = p.val;
                            if (!onlyIfAbsent)
                                p.val = value;
                        }
                    }
                }
            }
            //8、插入完键值对后再根据实际大小看是否需要转换成红黑树
            if (binCount != 0) {
                if (binCount >= TREEIFY_THRESHOLD)
                    treeifyBin(tab, i);
                if (oldVal != null)
                    return oldVal;
                break;
            }
        }
    }
    addCount(1L, binCount);
    return null;
}
```

**4、get()方法**

这里可以看到get() 方法是没有加锁的。Node中的val和next定义的时候用了volatile来保证可见性和有序性。

```java
public V get(Object key) {
    Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek;
    //1、计算hashcode
    int h = spread(key.hashCode());
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (e = tabAt(tab, (n - 1) & h)) != null) {//读取首节点的Node元素
        //2、如果该节点就是首节点则返回
        if ((eh = e.hash) == h) {
            if ((ek = e.key) == key || (ek != null && key.equals(ek)))
                return e.val;
        }
        //3、hash为负值代表正在扩容，如果当前链表已经迁移完成，那么头节点会被设置成fwd节点，此时get线程会帮助扩容。
        else if (eh < 0)
            return (p = e.find(h, key)) != null ? p.val : null;
        //4、既不是首节点也不是ForwardingNode，继续往下遍历
        while ((e = e.next) != null) {
            if (e.hash == h &&
                ((ek = e.key) == key || (ek != null && key.equals(ek))))
                return e.val;
        }
    }
    return null;
}
```

**5、扩容操作**

一般我们说的扩容，都包含两个步骤：

- 新建一个2倍大小的数组，这个过程要求单线程完成。
- 执行节点迁移，相当于把旧数组中所有节点重新"put"到新数组中。在1.8的HashMap中用了一个技巧，避免了重新根据hash值定位。根据这个我们可以知道，进行n -> 2n的扩容时，扩容前节点所在的hash桶的索引为index，这个节点迁移到新数组中只会有两种情况：要么还是在新数组的索引为index处，要么迁移到新数组的索引为index + n 的地方。所以旧的table数组上各个hash桶中的节点的迁移时不会相互影响的，这一点对多线程扩容非常有利。根据这一点，可以知道，每个hash桶的迁移都可以作为一个线程在扩容时的一个transfer任务。
- 另外，每个线程扩容都不应该规模太小(最小16个桶)，因为扩容不是IO型操作，节点迁移的执行速度本身很快，太多的线程来 执行节点迁移，线程调度开销占比变大，反而降低了吞吐量。ConcurrentHashMap这里，会根据CPU的核心数目，来算出一个transfer任务包含的hash桶的数量。

在扩容时，ConcurrentHashMap支持多线程并发扩容，在扩容过程中支持get查数据，若有线程put数据，还会帮助一起扩容，这种无阻塞算法，将并行最大化的设计。

![img](https://img-blog.csdn.net/20170305232906639?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMTM5Mjg5Nw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

```java
/**检查nextTable是否为null，如果是，则初始化nextTable，容量为table的两倍。
*自旋遍历节点，直到finished：节点从table复制到nextTable中，支持并发，思路如下： 
*如果节点 f 为null，则插入ForwardingNode（采用Unsafe.compareAndSwapObjectf方法实现），从而别的线 *程不会走到这个节点。
*如果f为链表的头节点（fh >= 0）,则先构造两个链表，通过hash值第n位不同区分(ph & n) == 0 ，然后把他们分*别放在nextTable的i和i + n位置，并将ForwardingNode 插入原节点位置，代表已经处理过了
*如果f为TreeBin节点，同样也是构造两课树 ，同时需要判断是否需要进行unTreeify()操作，并把处理的结果分别插*入到nextTable的i 和i+nw位置，并插入ForwardingNode 节点
*所有节点复制完成后，则将table指向nextTable，同时更新sizeCtl = nextTable的0.75倍，完成扩容过程
*/
private final void transfer(Node<K,V>[] tab, Node<K,V>[] nextTab) {
    int n = tab.length, stride;
    // 计算每个transfer任务中要负责迁移多少个hash桶
    if ((stride = (NCPU > 1) ? (n >>> 3) / NCPU : n) < MIN_TRANSFER_STRIDE)
        stride = MIN_TRANSFER_STRIDE; 
    if (nextTab == null) {            // initiating
        try {
            @SuppressWarnings("unchecked")
            //构造一个nextTable对象 它的容量是原来的两倍
            Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n << 1];
            nextTab = nt;
        } catch (Throwable ex) {      // try to cope with OOME
            sizeCtl = Integer.MAX_VALUE;
            return;
        }
        nextTable = nextTab;
        transferIndex = n;
    }
    int nextn = nextTab.length;
// 转发节点，在旧数组的一个hash桶中所有节点都被迁移完后，放置在这个hash桶中，表明已经迁移完，对它的读操作会转发到新数组
    ForwardingNode<K,V> fwd = new ForwardingNode<K,V>(nextTab);
    boolean advance = true;
    // 扩容中收尾的线程把做个值设置为true，进行本轮扩容的收尾工作（两件事，重新检查一次所有hash桶，给属性赋新值）
    boolean finishing = false; 
    for (int i = 0, bound = 0;;) {
        Node<K,V> f; int fh;
        while (advance) {
            int nextIndex, nextBound;
            if (--i >= bound || finishing)
                advance = false;
            else if ((nextIndex = transferIndex) <= 0) {
                i = -1;
                advance = false;
            }
            else if (U.compareAndSwapInt
                     (this, TRANSFERINDEX, nextIndex,
                      nextBound = (nextIndex > stride ?
                                   nextIndex - stride : 0))) {
                bound = nextBound;
                i = nextIndex - 1;
                advance = false;
            }
        }
        //如果所有的节点都已经完成复制工作  就把nextTable赋值给table
        if (i < 0 || i >= n || i + n >= nextn) {
            int sc;
            if (finishing) {
                nextTable = null;
                table = nextTab;
                //扩容阈值设置为原来容量的1.5倍  依然相当于现在容量的0.75倍
                sizeCtl = (n << 1) - (n >>> 1);
                return;
            }
            //利用CAS方法更新这个扩容阈值，在这里面sizectl值减一，说明新加入一个线程参与到扩容操作
            if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) {
                if ((sc - 2) != resizeStamp(n) << RESIZE_STAMP_SHIFT)
                    return;
                finishing = advance = true;
                i = n; 
            }
        }        
        //如果遍历到的节点为空 则放入ForwardingNode指针
        else if ((f = tabAt(tab, i)) == null)
            advance = casTabAt(tab, i, null, fwd);
        //如果遍历到ForwardingNode节点  说明这个点已经被处理过了 直接跳过，所以这就是能够并发的扩容！
        else if ((fh = f.hash) == MOVED)
            advance = true; // already processed
        else {
            synchronized (f) {
                if (tabAt(tab, i) == f) {// 判断下加锁的节点是hash桶中的第一个节点，加锁的是第一个节点才算加锁成功
                    Node<K,V> ln, hn;
                    if (fh >= 0) {
                        //因为n的值为数组的长度，且是power(2,x)，所以，在&操作的结果只可能是0或者n.
                        //根据这个规则,0-->  放在新表的相同位置,n-->  放在新表的（n+原来位置）
                        int runBit = fh & n;
                        // lastRun 表示的是需要复制的最后一个节点
                        Node<K,V> lastRun = f;
                        for (Node<K,V> p = f.next; p != null; p = p.next) {
                            int b = p.hash & n;
                            if (b != runBit) {
                                runBit = b;
                                lastRun = p;
                            }
                        }
                        if (runBit == 0) {
                            ln = lastRun;
                            hn = null;
                        }
                        else {
                            hn = lastRun;
                            ln = null;
                        }
                        // 构造两个链表，顺序大部分和原来是反的
                        //分别放到原来的位置和新增加的长度的相同位置(i/n+i)
                        for (Node<K,V> p = f; p != lastRun; p = p.next) {
                            int ph = p.hash; K pk = p.key; V pv = p.val;
                            if ((ph & n) == 0)
                                ln = new Node<K,V>(ph, pk, pv, ln);
                            else
                                hn = new Node<K,V>(ph, pk, pv, hn);
                        }
                        setTabAt(nextTab, i, ln);//在nextTable的i位置上插入一个链表
                        setTabAt(nextTab, i + n, hn);//在nextTable的i+n的位置上插入另一个链表
                        setTabAt(tab, i, fwd);//在table的i位置上插入forwardNode节点  表示已经处理过该节点
                        advance = true;//设置advance为true 返回到上面的while循环中 就可以执行i--操作
                    }
                    else if (f instanceof TreeBin) {
                        TreeBin<K,V> t = (TreeBin<K,V>)f;
                        TreeNode<K,V> lo = null, loTail = null;
                        TreeNode<K,V> hi = null, hiTail = null;
                        int lc = 0, hc = 0;
                        for (Node<K,V> e = t.first; e != null; e = e.next) {
                            int h = e.hash;
                            TreeNode<K,V> p = new TreeNode<K,V>
                                (h, e.key, e.val, null, null);
                            if ((h & n) == 0) {
                                if ((p.prev = loTail) == null)
                                    lo = p;
                                else
                                    loTail.next = p;
                                loTail = p;
                                ++lc;
                            }
                            else {
                                if ((p.prev = hiTail) == null)
                                    hi = p;
                                else
                                    hiTail.next = p;
                                hiTail = p;
                                ++hc;
                            }
                        }
                        ln = (lc <= UNTREEIFY_THRESHOLD) ? untreeify(lo) :
                            (hc != 0) ? new TreeBin<K,V>(lo) : t;
                        hn = (hc <= UNTREEIFY_THRESHOLD) ? untreeify(hi) :
                            (lc != 0) ? new TreeBin<K,V>(hi) : t;
                        setTabAt(nextTab, i, ln);
                        setTabAt(nextTab, i + n, hn);
                        setTabAt(tab, i, fwd);
                        advance = true;
                    }
                }
            }
        }
    }
}
```

**6、总结**

1.ConcurrentHashMap是HashMap的线程安全版本。

2.ConcurrentHashMap底层数据结构为**数组+链表+红黑树**，默认容量为16，不允许[key,value]为null。

3.ConcurrentHashMap内部采用的锁有synchronized、CAS、自旋锁、分段锁、volatile。

4.通过sizeCtl变量来控制扩容、初始化等操作。

5.查询操作不加锁，因此ConcurrentHashMap**不是强一致性**。

**ConcurrentHashMap如何保证线程安全？**

CAS+对节点加锁（减小锁粒度）+volatile

1、Node节点中的value和next指针使用了volatile来保证其可见性；table变量使用了volatile来保证每次获取到的都是最新写入的值。

2、初始化时，防止多线程初始化：

- volatile变量（sizeCtl）：它是一个标记位，用来告诉其他线程这个坑位有没有人在，其线程间的可见性由volatile保证。
- CAS操作：CAS操作保证了设置sizeCtl标记位的原子性，保证了只有一个线程能设置成功

```java
private final Node<K,V>[] initTable() {
  Node<K,V>[] tab; int sc;
  //每次循环都获取最新的Node数组引用
  while ((tab = table) == null || tab.length == 0) {
    //sizeCtl是一个标记位，若为-1也就是小于0，代表有线程在进行初始化工作了
    if ((sc = sizeCtl) < 0)
      //让出CPU时间片
      Thread.yield(); // lost initialization race; just spin
    //CAS操作，将本实例的sizeCtl变量设置为-1
    else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {
      //如果CAS操作成功了，代表本线程将负责初始化工作
      try {
        //再检查一遍数组是否为空
        if ((tab = table) == null || tab.length == 0) {
          //在初始化Map时，sizeCtl代表数组大小，默认16
          //所以此时n默认为16
          int n = (sc > 0) ? sc : DEFAULT_CAPACITY;
          @SuppressWarnings("unchecked")
          //Node数组
          Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];
          //将其赋值给table变量
          table = tab = nt;
          //通过位运算，n减去n二进制右移2位，相当于乘以0.75
          //例如16经过运算为12，与乘0.75一样，只不过位运算更快
          sc = n - (n >>> 2);
        }
      } finally {
        //将计算后的sc（12）直接赋值给sizeCtl，表示达到12长度就扩容
        //由于这里只会有一个线程在执行，直接赋值即可，没有线程安全问题
        //只需要保证可见性
        sizeCtl = sc;
      }
      break;
    }
  }
  return tab;
}
```

3、put操作时

采用tabAt(tab, i)方法，其使用Unsafe类volatile的操作volatile式地查看值，保证每次获取到的值都是最新的。

利用CAS尝试写入，失败则自旋保证成功。

4、get操作时

Node中的val和next定义的时候用了volatile来保证可见性和有序性。

5、扩容操作时















